{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import pickle\n",
    "import pandas_market_calendars as mcal\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "from gymnasium import spaces\n",
    "from mmd.env import GenLSTM, MMDSimulator, load_generator\n",
    "from mmd.train import start_writer, get_params_from_events, get_params_dicts, get_robustq_params_dicts, train_robustdqn\n",
    "from mmd.evaluation import simulate_agent_spx\n",
    "from agent.q import QFunc\n",
    "from agent.DQN import PORDQN, DQN, PORDQN_Nadam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0 # results in the paper used 0, 1, 2, 3, 4\n",
    "\n",
    "total_length = 560\n",
    "burn_in = 500\n",
    "state_len = 60\n",
    "cal_start_date = '1995-01-01'\n",
    "cal_end_date = '2024-12-31'\n",
    "trading_calendar = 'NYSE'\n",
    "calendar = mcal.get_calendar(trading_calendar)\n",
    "schedule = calendar.schedule(start_date=cal_start_date, end_date=cal_end_date)\n",
    "\n",
    "int_rate = 0.024\n",
    "trans_cost = 0.0005 # standard cost = 0.0005\n",
    "eval_batch_size = 1000\n",
    "eval_seed = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/mmd_generator/ma_params.pkl', 'rb') as f:\n",
    "    ma_model_params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_path = './data/mmd_generator/'\n",
    "params = get_params_from_events(events_path)\n",
    "for key, value in params.items():\n",
    "    for key, value in value.items():\n",
    "        if key in globals(): continue # skip if already in globals\n",
    "        globals()[key] = value\n",
    "data_params, model_params, train_params = get_params_dicts(vars().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = GenLSTM(noise_dim, seq_dim, sample_len, hidden_size=hidden_size, n_lstm_layers=n_lstm_layers, activation=activation)\n",
    "generator = load_generator(generator, events_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "\n",
    "# simulator params\n",
    "batch_size = 8\n",
    "device = 'cpu'\n",
    "action_space = spaces.Discrete(9)\n",
    "action_values = torch.linspace(-1., 1., 9)\n",
    "env = MMDSimulator(generator, ma_model_params, trading_calendar, cal_start_date, cal_end_date, state_len, burn_in, batch_size=batch_size, action_space=action_space, action_values=action_values)\n",
    "other_state_vars = ['log_wealth', 'positions', 'dt']\n",
    "\n",
    "# model params\n",
    "architecture = [64, 64]\n",
    "obs_dim = state_len+len(other_state_vars)\n",
    "num_actions = action_values.shape[0]\n",
    "discount = 0.99\n",
    "qfunc = QFunc(state_len+len(other_state_vars), architecture, action_values.shape[0])\n",
    "eps_greedy = 0.1 # Epsilon-greedy exploration\n",
    "buffer_max_length = int(1e5)\n",
    "clone_steps = 50\n",
    "train_steps = 1\n",
    "agent_batch_size = 128\n",
    "n_batches = 1\n",
    "n_epochs = 1\n",
    "lr = 1e-4\n",
    "n_episodes = 5\n",
    "\n",
    "dqn_agent = DQN(obs_dim, num_actions, discount, qfunc, eps_greedy, buffer_max_length, clone_steps, train_steps, agent_batch_size, n_batches, n_epochs, lr, device=device, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(n_episodes)):\n",
    "    obs, _ = env.reset()\n",
    "    done = torch.tensor([False]*batch_size)\n",
    "    action = dqn_agent.agent_start(obs)\n",
    "    cum_reward = torch.zeros(batch_size)\n",
    "    while not done.any():\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        cum_reward += reward.squeeze()\n",
    "        if done.any():\n",
    "            dqn_agent.agent_end(reward, obs)\n",
    "            break\n",
    "        else:\n",
    "            action = dqn_agent.agent_step(reward, obs)\n",
    "    print(f'Cumulative reward: {cum_reward.mean().item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.training_mode = False\n",
    "torch.manual_seed(eval_seed)\n",
    "dqn_eval_env = MMDSimulator(generator, ma_model_params, trading_calendar, cal_start_date, cal_end_date, state_len, burn_in, batch_size=eval_batch_size, logging=True, action_space=action_space, action_values=action_values)\n",
    "obs, reset_info = dqn_eval_env.reset()\n",
    "action = dqn_agent.get_action(obs)\n",
    "done = torch.tensor([False] * batch_size)\n",
    "while not done.any():\n",
    "    obs, reward, done, truncated, info = dqn_eval_env.step(action)\n",
    "    if done.any():\n",
    "        break\n",
    "    else:\n",
    "        action = dqn_agent.get_action(obs)\n",
    "dqn_eval_env.print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = simulate_agent_spx(dqn_agent.q, action_values, int_rate, trans_cost)\n",
    "for key, value in metrics.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "batch_size = 8\n",
    "device = 'cpu'\n",
    "action_space = spaces.Discrete(9)\n",
    "action_values = torch.linspace(-1., 1., 9)\n",
    "num_actions = len(action_values)\n",
    "nu_dist = 't'\n",
    "nu_scale = 0.03\n",
    "nu_df = 2\n",
    "other_state_vars = ['log_wealth', 'positions', 'dt']\n",
    "obs_dim = state_len + len(other_state_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN IF TRAINING FROM SCRATCH\n",
    "discount = 0.99\n",
    "eps_greedy = 0.1 # epsilon greedy parameter\n",
    "buffer_max_length = int(1e5)\n",
    "clone_steps = 50\n",
    "train_steps = 1\n",
    "agent_batch_size = 128\n",
    "n_batches = 1\n",
    "n_epochs = 1\n",
    "robustq_lr = 1e-4\n",
    "architecture = [64, 64]\n",
    "pre_train_Q = False\n",
    "n_episodes = 5\n",
    "\n",
    "robustq = QFunc(state_len+len(other_state_vars), architecture, action_values.shape[0]).to(device)\n",
    "\n",
    "delta = 1e-4 # regularisation parameter for Sinkhorn distance\n",
    "epsilon = 0.003 # Sinkhorn distance\n",
    "norm_ord = 1\n",
    "lamda_init = 0. # initial lambda\n",
    "lamda_max_iter = 100\n",
    "lamda_step_size = 10 # step size for learning rate scheduler\n",
    "lamda_gamma = 10. # gamma for learning rate scheduler\n",
    "lamda_lr = 0.02 # learning rate for lambda\n",
    "n_outer = 1 # not used in this algorithm but used in logging by writer\n",
    "n_inner = 1000 # number of samples from nu to calc inner expectations\n",
    "\n",
    "simulator_params, model_params = get_robustq_params_dicts(vars().copy())\n",
    "writer = start_writer(simulator_params, model_params, model_name='PORDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MMDSimulator(generator, ma_model_params, trading_calendar, cal_start_date, cal_end_date, state_len, burn_in,int_rate, trans_cost, batch_size, action_space, action_values, device)\n",
    "\n",
    "robustdqn_agent = PORDQN(obs_dim, num_actions, discount, nu_scale, nu_df, action_values, epsilon, delta, n_inner, lamda_init,lamda_lr, lamda_max_iter, lamda_step_size, lamda_gamma, norm_ord, robustq, eps_greedy, buffer_max_length, clone_steps, train_steps, agent_batch_size, n_batches, n_epochs, robustq_lr, device=device, seed=seed, writer=writer)\n",
    "\n",
    "robustdqn_agent = train_robustdqn(robustdqn_agent, env, writer, simulator_params, model_params)\n",
    "\n",
    "# Storing agent to non-volatile memory\n",
    "with open('robustdqn_agent_q.pkl', 'wb') as f:\n",
    "    pickle.dump(robustdqn_agent.q, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading agent from non-volatile memory\n",
    "robustdqn_agent_q: PORDQN\n",
    "with open('robustdqn_agent_q.pkl', 'rb') as f:\n",
    "    robustdqn_agent_q = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robustdqn_agent.training_mode = False\n",
    "torch.manual_seed(eval_seed)\n",
    "dqn_eval_env = MMDSimulator(generator, ma_model_params, trading_calendar, cal_start_date, cal_end_date, state_len, burn_in, batch_size=eval_batch_size, logging=True, action_space=action_space, action_values=action_values, int_rate=int_rate, trans_cost=trans_cost)\n",
    "obs, reset_info = dqn_eval_env.reset()\n",
    "action = robustdqn_agent.get_action(obs)\n",
    "done = torch.tensor([False] * batch_size)\n",
    "while not done.any():\n",
    "    obs, reward, done, truncated, info = dqn_eval_env.step(action)\n",
    "    if done.any():\n",
    "        break\n",
    "    else:\n",
    "        action = robustdqn_agent.get_action(obs)\n",
    "dqn_eval_env.print_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_agent_spx('data/spx.csv', robustdqn_agent_q, action_values, int_rate=int_rate, trans_cost=trans_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust DQN with NAdam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "batch_size = 8\n",
    "device = 'cpu'\n",
    "action_space = spaces.Discrete(9)\n",
    "action_values = torch.linspace(-1., 1., 9)\n",
    "num_actions = len(action_values)\n",
    "nu_dist = 't'\n",
    "nu_scale = 0.03\n",
    "nu_df = 2\n",
    "other_state_vars = ['log_wealth', 'positions', 'dt']\n",
    "obs_dim = state_len + len(other_state_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN IF TRAINING FROM SCRATCH\n",
    "discount = 0.99\n",
    "eps_greedy = 0.1 # epsilon greedy parameter\n",
    "buffer_max_length = int(1e5)\n",
    "clone_steps = 50\n",
    "train_steps = 1\n",
    "agent_batch_size = 128\n",
    "n_batches = 1\n",
    "n_epochs = 1\n",
    "robustq_lr = 1e-4\n",
    "architecture = [64, 64]\n",
    "pre_train_Q = False\n",
    "n_episodes = 5\n",
    "\n",
    "robustq = QFunc(state_len+len(other_state_vars), architecture, action_values.shape[0]).to(device)\n",
    "\n",
    "delta = 1e-4 # regularisation parameter for Sinkhorn distance\n",
    "epsilon = 0.003 # Sinkhorn distance\n",
    "norm_ord = 1\n",
    "lamda_init = 0. # initial lambda\n",
    "lamda_max_iter = 100\n",
    "lamda_step_size = 10 # step size for learning rate scheduler\n",
    "lamda_gamma = 10. # gamma for learning rate scheduler\n",
    "lamda_lr = 0.02 # learning rate for lambda\n",
    "n_outer = 1 # not used in this algorithm but used in logging by writer\n",
    "n_inner = 1000 # number of samples from nu to calc inner expectations\n",
    "\n",
    "simulator_params, model_params = get_robustq_params_dicts(vars().copy())\n",
    "writer = start_writer(simulator_params, model_params, model_name='PORDQN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MMDSimulator(generator, ma_model_params, trading_calendar, cal_start_date, cal_end_date, state_len, burn_in,int_rate, trans_cost, batch_size, action_space, action_values, device)\n",
    "\n",
    "robustdqn_agent = PORDQN_Nadam(obs_dim, num_actions, discount, nu_scale, nu_df, action_values, epsilon, delta, n_inner, lamda_init,lamda_lr, lamda_max_iter, lamda_step_size, lamda_gamma, norm_ord, robustq, eps_greedy, buffer_max_length, clone_steps, train_steps, agent_batch_size, n_batches, n_epochs, robustq_lr, device=device, seed=seed, writer=writer)\n",
    "\n",
    "robustdqn_agent = train_robustdqn(robustdqn_agent, env, writer, simulator_params, model_params)\n",
    "\n",
    "# Storing agent to non-volatile memory\n",
    "with open('robustdqn_NAdam_agent_q.pkl', 'wb') as f:\n",
    "    pickle.dump(robustdqn_agent.q, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading agent from non-volatile memory\n",
    "robustdqn_agent_q: PORDQN\n",
    "with open('robustdqn_NAdam_agent_q.pkl', 'rb') as f:\n",
    "    robustdqn_agent_q = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_agent_spx('data/spx.csv', robustdqn_agent_q, action_values, int_rate=int_rate, trans_cost=trans_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusted volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulate_agent_spx('data/spx_volatility_adjusted.csv', robustdqn_agent_q, action_values, int_rate=int_rate, trans_cost=trans_cost)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "robustq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
